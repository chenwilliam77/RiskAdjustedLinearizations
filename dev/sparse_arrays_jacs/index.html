<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sparse Arrays and Jacobians · RiskAdjustedLinearizations.jl</title><link rel="canonical" href="https://juliadocs.github.io/Documenter.jl/stable/sparse_arrays_jacs/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">RiskAdjustedLinearizations.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../risk_adjusted_linearization/">Risk-Adjusted Linearizations</a></li><li class="is-active"><a class="tocitem" href>Sparse Arrays and Jacobians</a><ul class="internal"><li><a class="tocitem" href="#Sparsity-with-\\Gamma_5,-\\Gamma_6,-\\Lambda,-and-\\Sigma-1"><span>Sparsity with <span>$\Gamma_5$</span>, <span>$\Gamma_6$</span>, <span>$\Lambda$</span>, and <span>$\Sigma$</span></span></a></li><li><a class="tocitem" href="#Sparse-Jacobians-and-Automatic-Differentiation-1"><span>Sparse Jacobians and Automatic Differentiation</span></a></li></ul></li><li><a class="tocitem" href="../numerical_algorithms/">Numerical Algorithms</a></li><li><a class="tocitem" href="../example/">Example</a></li><li><a class="tocitem" href="../caching/">Caching</a></li><li><a class="tocitem" href="../diagnostics/">Diagnostics</a></li><li><a class="tocitem" href="../tips/">Tips</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Sparse Arrays and Jacobians</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sparse Arrays and Jacobians</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/master/docs/src/sparse_arrays_jacs.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sparse-arrays-jacs-1"><a class="docs-heading-anchor" href="#sparse-arrays-jacs-1">Sparse Arrays and Jacobians</a><a class="docs-heading-anchor-permalink" href="#sparse-arrays-jacs-1" title="Permalink"></a></h1><p>The risk-adjusted linearization of many economic models contains substantial amounts of sparsity. The matrices <span>$\Gamma_5$</span> and <span>$\Gamma_6$</span> as well as the output of the functions <span>$\Lambda(\cdot)$</span> and <span>$\Sigma(\cdot)$</span> are typically sparse. All of the Jacobians, <span>$\Gamma_1$</span>, <span>$\Gamma_2$</span>, <span>$\Gamma_3$</span>, <span>$\Gamma_4$</span>, and <span>$J\mathcal{V}$</span>, are also very sparse. To optimize performance, RiskAdjustedLinearizations.jl allows users to leverage the sparsity of these objects. The caches for the first set of objects can be sparse matrices, assuming that <span>$\Lambda(\cdot)$</span> and <span>$\Sigma(\cdot)$</span> are written properly. The second set of objects are usually computed with forward-mode automatic differentiation. By using matrix coloring techniques implemented by <a href="https://github.com/JuliaDiff/SparseDiffTools.jl">SparseDiffTools</a>, we can accelerate the calculation of these Jacobians and cache their output as sparse matrices.</p><p>These methods can be easily used through keyword arguments of the main constructor of the <code>RiskAdjustedLinearization</code> type. We have also written examples which show how to use these methods and time their speed. See the folder <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods">examples/sparse_methods</a>. The script <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods/sparse_arrays_and_jacobians.jl">sparse_arrays_and_jacobians.jl</a> illustrates how to apply the methods described in this documentation page while <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods/sparse_nlsolve_jacobians.jl">sparse_nlsolve_jacobians.jl</a> describe how to use sparse automatic differentiation to accelerate the calculation of Jacobians during calls to <code>nlsolve</code>. See <a href="../numerical_algorithms/#numerical-algorithms-1">Numerical Algorithms</a> for more details on the latter. Finally, the script <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods/combined_sparse_methods.jl">combined_sparse_methods.jl</a> combines these methods to achieve the fastest possible speeds with this package.</p><h2 id="Sparsity-with-\\Gamma_5,-\\Gamma_6,-\\Lambda,-and-\\Sigma-1"><a class="docs-heading-anchor" href="#Sparsity-with-\\Gamma_5,-\\Gamma_6,-\\Lambda,-and-\\Sigma-1">Sparsity with <span>$\Gamma_5$</span>, <span>$\Gamma_6$</span>, <span>$\Lambda$</span>, and <span>$\Sigma$</span></a><a class="docs-heading-anchor-permalink" href="#Sparsity-with-\\Gamma_5,-\\Gamma_6,-\\Lambda,-and-\\Sigma-1" title="Permalink"></a></h2><p>The matrices <span>$\Gamma_5$</span> and <span>$\Gamma_6$</span> are constants and can be passed in directly as sparse matrices. The caches for <span>$\Lambda$</span> and <span>$\Sigma$</span> can be initialized as sparse matrices by using the <code>Λ_cache_init</code> and <code>Σ_cache_init</code> keywords for <code>RiskAdjustedLinearization</code>. This keyword is a function which takes as input a <code>Tuple</code> of <code>Int</code> dimensions and allocates an array with those dimensions. By default, these keyword have the values</p><pre><code class="language-none">Λ_cache_init = dims -&gt; Matrix{Float64}(undef, dims...)
Σ_cache_init = dims -&gt; Matrix{Float64}(undef, dims...)</code></pre><p>To use <code>SparseMatrixCSC</code> arrays, the user would instead pass</p><pre><code class="language-none">Λ_cache_init = dims -&gt; spzeros(dims...)
Σ_cache_init = dims -&gt; spzeros(dims...)</code></pre><p>However, the user should be aware of two caveats.</p><ol><li>Using sparse arrays for caches may not always be faster</li></ol><p>for calculating the steady state. To obtain <span>$\Psi$</span>, we need to apply the Schur decomposition, which requires dense matrices. Thus, we still have to allocate dense versions of the sparse caches.</p><ol><li>If <span>$\Lambda$</span> is nonzero, then the cache for <span>$\Sigma$</span> cannot be sparse.</li></ol><p>The reason is that we need to compute <code>(I - Λ * Ψ) \ Σ</code>, but this calculation will fail when the <code>Σ</code> is sparse. The cache, however, can be other special matrix types as long as the left division works. For example, the matrix could be a <code>Diagonal</code> or <code>BandedMatrix</code>.</p><h2 id="Sparse-Jacobians-and-Automatic-Differentiation-1"><a class="docs-heading-anchor" href="#Sparse-Jacobians-and-Automatic-Differentiation-1">Sparse Jacobians and Automatic Differentiation</a><a class="docs-heading-anchor-permalink" href="#Sparse-Jacobians-and-Automatic-Differentiation-1" title="Permalink"></a></h2><p>To calculate a risk-adjusted linearization, we need to compute the Jacobians of <span>$\mu$</span> and <span>$\xi$</span> with respect to <span>$z$</span> and <span>$y$</span> as well as the Jacobian of <span>$\mathcal{V}$</span> with respect to <span>$z$</span>. These Jacobians are typically sparse because each equation in economic models only has a small subset of variables. To exploit this sparsity, we utilize methods from <a href="https://github.com/JuliaDiff/SparseDiffTools.jl">SparseDiffTools.jl</a>.</p><p>There are two ways to instruct a <code>RiskAdjustedLinearization</code> that the Jacobians of <span>$\mu$</span>, <span>$\xi$</span>, and/or <span>$\mathcal{V}$</span> are sparse. The first applies during the construction of an instance while the second occurs after an instance exists.</p><p>Note that sparse differentiation for this package is still a work in progress. While working examples exist, the code still has bugs. The major problems are listed below:</p><ul><li>Homotopy does not work yet with sparse automatic differentiation.</li><li><code>NaN</code>s or undefined values sometimes occur during calls to <code>nlsolve</code> within <code>solve!</code>. However,  the numerical algorithm can succeed if <code>solve!</code> is repeatedly run, even when using the same initial guess for the coefficients <span>$(z, y, \Psi)$</span>. This happens at a sufficiently high frequency that using sparse automatic differentiation is not reliable.</li></ul><h3 id="Specify-Sparsity-during-Construction-1"><a class="docs-heading-anchor" href="#Specify-Sparsity-during-Construction-1">Specify Sparsity during Construction</a><a class="docs-heading-anchor-permalink" href="#Specify-Sparsity-during-Construction-1" title="Permalink"></a></h3><p>When constructing a <code>RiskAdjustedLinearization</code>, the keyword <code>sparse_jacobian::Vector{Symbol}</code> is a vector containing the symbols <code>:μ</code>, <code>:ξ</code>, and/or <code>:𝒱</code>. For example, if</p><pre><code class="language-none">sparse_jacobian = [:μ, :𝒱]</code></pre><p>then the constructor will interpret that <span>$\mu$</span> has sparse Jacobians with respect to <span>$z$</span> and <span>$y$</span>, and that <span>$\mathcal{V}$</span> has a sparse Jacobian with respect to <span>$z$</span>.</p><p>To implement sparse differentiation, the user needs to provide a sparsity pattern and a matrix coloring vector. The user can use the keywords <code>sparsity</code> and <code>colorvec</code> to provide this information. These keywords are dictionaries whose keys are the names of the Jacobians and values are the sparsity pattern and matrix coloring vector. The relevant keys are <code>:μz</code>, <code>:μy</code>, <code>:ξz</code>, <code>:ξy</code>, and <code>:J𝒱</code>, where</p><ul><li><code>:μz</code> and <code>:μy</code> are the Jacobians of <code>μ</code> with respect to <span>$z$</span> and <span>$y$</span>,</li><li><code>:ξz</code> and <code>:ξy</code> are the Jacobians of <code>ξ</code> with respect to <span>$z$</span> and <span>$y$</span>, and</li><li><code>:J𝒱</code> is the Jacobian of <code>𝒱</code> with respect to <span>$z$</span>.</li></ul><p>If <code>sparse_jacobian</code> is nonempty, but one of these dictionaries is empty or does not contain the correct subset of the keys <code>:μz</code>, <code>:μy</code>, <code>:ξz</code>, <code>:ξy</code>, and <code>:J𝒱</code>, then we attempt to determine the sparsity pattern and/or matrix coloring vector. Once the sparsity pattern is known, the matrix coloring vector is determined by calling <code>matrix_colors</code>. We implement two approaches to discern the sparsity pattern. By default, we compute the dense Jacobian once using ForwardDiff and assume that any zeros in the computed Jacobian are supposed to be zero. If this assumption is true, then this Jacobian can be used as the sparsity pattern. Alternatively, the user can set the keyword <code>sparsity_detection = true</code>, in which case we call <code>jacobian_sparsity</code> from <a href="https://github.com/SciML/SparsityDetection.jl">SparsityDetection.jl</a>. to determine the sparsity pattern. Currently, only the first approach works.</p><p>For <span>$\mu$</span> and <span>$\xi$</span>, the first approach typically works fine. For <span>$\mathcal{V}$</span>, however, if the user guesses that <span>$\Psi$</span> is a matrix of zeros, then the Jacobian will be zero as well. A good guess of <span>$\Psi$</span> is crucial to inferring the correct sparsity pattern of <span>$\mathcal{V}$</span> because different <span>$\Psi$</span> can imply different sparsity patterns. For this reason, to fully exploit the sparsity in a model, we recommend calculating the risk-adjusted linearization once using dense Jacobian methods. The calculated Jacobians can be used subsequently as the sparsity patterns.</p><p>For reference, see the <a href="../risk_adjusted_linearization/#implement-ral-1">docstring for <code>RiskAdjustedLinearization</code></a>.</p><h3 id="Update-a-RiskAdjustedLinearization-with-Sparse-Jacobians-after-Construction-1"><a class="docs-heading-anchor" href="#Update-a-RiskAdjustedLinearization-with-Sparse-Jacobians-after-Construction-1">Update a <code>RiskAdjustedLinearization</code> with Sparse Jacobians after Construction</a><a class="docs-heading-anchor-permalink" href="#Update-a-RiskAdjustedLinearization-with-Sparse-Jacobians-after-Construction-1" title="Permalink"></a></h3><p>Sparse Jacobians can be specified after a <code>RiskAdjustedLinearization</code> object <code>m</code> already exists by calling <code>update_sparsity_pattern!(m, function_names)</code>. The syntax of <code>update_sparsity_pattern!</code> is very similar to the specification of sparse Jacobians in the constructor. The second input <code>function_names</code> is either a <code>Symbol</code> or <code>Vector{Symbol}</code>, and it specifies the Jacobian(s) whose sparsity pattern(s) should be updated. The relevent symbols are <code>:μz</code>, <code>:μy</code>, <code>:ξz</code>, <code>:ξy</code>, and <code>:J𝒱</code>. If the Jacobians calculated by <code>m</code> are dense Jacobians, then <code>update_sparsity_pattern!</code> will replace the functions computing dense Jacobians with functions that exploit sparsity. If the Jacobians are already being calculated as sparse Jacobians, then <code>update_sparsity_pattern!</code> can update the sparsity pattern and matrix coloring vector being used.</p><p>If no keywords are passed, then <code>update_sparsity_pattern!</code> will use the same methods as the constructor to infer the sparsity pattern. Either we compute the dense Jacobian once using ForwardDiff, or we utilize SparsityDetection. The new sparsity pattern and matrix coloring vectors can be specified using the <code>sparsity</code> and <code>colorvec</code> keywords, just like the constructor. Different values for <span>$z$</span>, <span>$y$</span>, and <span>$\Psi$</span> can also be used when trying to infer the sparsity pattern by passing the new values as keywords.</p><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.update_sparsity_pattern!" href="#RiskAdjustedLinearizations.update_sparsity_pattern!"><code>RiskAdjustedLinearizations.update_sparsity_pattern!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">update_sparsity_pattern!(m::RiskAdjustedLinearization, function_name::Union{Symbol, Vector{Symbol}};
                         z::AbstractVector{&lt;: Number} = m.z,
                         y::AbstractVector{&lt;: Number} = m.y,
                         Ψ::AbstractVector{&lt;: Number} = m.Ψ,
                         sparsity::AbstractDict{Symbol, AbstractMatrix} = Dict{Symbol, AbstractMatrix}(),
                         colorvec::AbstractDict{Symbol, &lt;: AbstractVector{Int}} = Dict{Symbol, Vector{Int}}(),
                         sparsity_detection::Bool = false)</code></pre><p>updates the Jacobians of μ, ξ, and/or 𝒱 in <code>m</code> with a new sparsity pattern. The Jacobians to be updated are specified by <code>function_name</code>, e.g. <code>function_name = [:μ, :ξ, :𝒱]</code>.</p><p>If the keyword <code>sparsity</code> is empty, then the function attempts to determine the new sparsity pattern by computing the Jacobian via automatic differentiation and assuming any zeros are supposed to be zero. Keywords provide guesses for the coefficients <span>$(z, y, \Psi)$</span> that are required to calculate the Jacobians.</p><p><strong>Keywords</strong></p><ul><li><code>z</code>: state coefficients at steady state</li><li><code>y</code>: jump coefficients at steady state</li><li><code>Ψ</code>: coefficients for mapping from states to jumps</li><li><code>sparsity</code>: key-value pairs can be used to specify new sparsity patterns for the Jacobian functions   <code>μz</code>, <code>μy</code>, <code>ξz</code>, <code>ξy</code>, and <code>J𝒱</code>.</li><li><code>colorvec</code>: key-value pairs can be used to specify new matrix coloring vectors for the Jacobian functions   <code>μz</code>, <code>μy</code>, <code>ξz</code>, <code>ξy</code>, and <code>J𝒱</code>.</li><li><code>sparsity_detection</code>: use SparsityDetection.jl to determine the sparsity pattern.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/01588642d4ea41665146a8956336987a31b0c07f/src/sparse_jacobian_helpers.jl#L42-L69">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../risk_adjusted_linearization/">« Risk-Adjusted Linearizations</a><a class="docs-footer-nextpage" href="../numerical_algorithms/">Numerical Algorithms »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 21 February 2021 22:47">Sunday 21 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
