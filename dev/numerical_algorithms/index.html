<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Numerical Algorithms · RiskAdjustedLinearizations.jl</title><link rel="canonical" href="https://juliadocs.github.io/Documenter.jl/stable/numerical_algorithms/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">RiskAdjustedLinearizations.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../risk_adjusted_linearization/">Risk-Adjusted Linearizations</a></li><li><a class="tocitem" href="../sparse_arrays_jacs/">Sparse Arrays and Jacobians</a></li><li class="is-active"><a class="tocitem" href>Numerical Algorithms</a><ul class="internal"><li><a class="tocitem" href="#solve!-1"><span><code>solve!</code></span></a></li><li><a class="tocitem" href="#relaxation-1"><span>Relaxation</span></a></li><li><a class="tocitem" href="#homotopy-1"><span>Homotopy Continuation</span></a></li><li><a class="tocitem" href="#blanchard-kahn-1"><span>Blanchard-Kahn Conditions</span></a></li><li><a class="tocitem" href="#sparsity-numerical-algo-1"><span>Exploiting Sparsity</span></a></li><li><a class="tocitem" href="#Docstrings-1"><span>Docstrings</span></a></li></ul></li><li><a class="tocitem" href="../example/">Example</a></li><li><a class="tocitem" href="../caching/">Caching</a></li><li><a class="tocitem" href="../diagnostics/">Diagnostics</a></li><li><a class="tocitem" href="../tips/">Tips</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Numerical Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Numerical Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/master/docs/src/numerical_algorithms.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="numerical-algorithms-1"><a class="docs-heading-anchor" href="#numerical-algorithms-1">Numerical Algorithms</a><a class="docs-heading-anchor-permalink" href="#numerical-algorithms-1" title="Permalink"></a></h1><p>To calculate the risk-adjusted linearization, we need to solve a system of nonlinear equations. These equations are generally solvable using Newton-type methods. The package currently has two available algorithms, <a href="#relaxation-1">relaxation</a> and <a href="#homotopy-1">homotopy continuation</a></p><h2 id="solve!-1"><a class="docs-heading-anchor" href="#solve!-1"><code>solve!</code></a><a class="docs-heading-anchor-permalink" href="#solve!-1" title="Permalink"></a></h2><p>The primary interface for calculating a risk-adjusted linearization once a <code>RiskAdjustedLinearization</code> object is created is the function <code>solve!</code>. The user selects the desired numerical algorithm through <code>algorithm</code> keyword of <code>solve!</code>.</p><p>All of the available algorithms need to solve a system of nonlinear equations. We use <code>nlsolve</code> for this purpose, and all keyword arguments for <code>nlsolve</code> can be passed as keyword arguments to <code>solve!</code>, e.g. <code>autodiff</code> and <code>ftol</code>. The user can also exploit sparsity in the Jacobian of the system of nonlinear equations to accelerate <code>nlsolve</code> by using the keywords <code>sparse_jacobian</code>, <code>sparsity</code>, <code>colorvec</code>, <code>jac_cache</code>, and/or <code>sparsity_detection</code>. For details, see <a href="#sparsity-numerical-algo-1">Exploiting Sparsity</a>.</p><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.solve!" href="#RiskAdjustedLinearizations.solve!"><code>RiskAdjustedLinearizations.solve!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">solve!(m; algorithm = :relaxation, autodiff = :central, verbose = :high, kwargs...)
solve!(m, z0, y0; kwargs...)
solve!(m, z0, y0, Ψ0; kwargs...)</code></pre><p>computes the risk-adjusted linearization of the dynamic economic model described by <code>m</code> and updates <code>m</code> with the solution, e.g. the coefficients <span>$(z, y, \Psi)$</span>.</p><p>The three available <code>solve!</code> algorithms are slight variations on each other.</p><ul><li><p>Method 1: uses the <code>z</code>, <code>y</code>, and <code>Ψ</code> fields of <code>m</code> as initial guesses   for <span>$(z, y, \Psi)$</span> and proceeds with the numerical algorithm   specified by <code>algorithm</code></p></li><li><p>Method 2: uses <code>z0</code> and <code>y0</code> as initial guesses for the deterministic   steady state, which is then used as the initial guess for <span>$(z, Y, \Psi)$</span>   for the numerical algorithm specified by <code>algorithm</code>.</p></li><li><p>Method 3: uses <code>z0</code>, <code>y0</code>, and <code>Ψ0</code> as initial guesses for <span>$(z, Y, \Psi)$</span>   and proceeds with the numerical algorithm specified by <code>algorithm</code>.</p></li></ul><p><strong>Inputs</strong></p><ul><li><code>m::RiskAdjustedLinearization</code>: object holding functions needed to calculate   the risk-adjusted linearization</li><li><code>z0::AbstractVector{S1}</code>: initial guess for <span>$z$</span></li><li><code>y0::AbstractVector{S1}</code>: initial guess for <span>$y$</span></li><li><code>Ψ0::AbstractVector{S1}</code>: initial guess for <span>$\Psi$</span></li><li><code>S1 &lt;: Real</code></li></ul><p><strong>Keywords</strong></p><ul><li><code>algorithm::Symbol = :relaxation</code>: which numerical algorithm to use? Can be one of <code>[:relaxation, :homotopy, :deterministic]</code></li><li><code>autodiff::Symbol = :central</code>: use autodiff or not? This keyword is the same as in <code>nlsolve</code></li><li><code>use_anderson::Bool = false</code>: use Anderson acceleration if the relaxation algorithm is applied. Defaults to <code>false</code></li><li><code>step::Float64 = .1</code>: size of step from 0 to 1 if the homotopy algorithm is applied. Defaults to 0.1</li><li><code>sparse_jacobian::Bool = false</code>: if true, exploit sparsity in the Jacobian in calls to <code>nlsolve</code> using SparseDiffTools.jl.   If <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>, then <code>solve!</code> will attempt to determine the sparsity pattern.</li><li><code>sparsity::Union{AbstractArray, Nothing} = nothing</code>: sparsity pattern for the Jacobian in calls to <code>nlsolve</code></li><li><code>colorvec = nothing</code>: matrix coloring vector for sparse Jacobian in calls to <code>nlsolve</code></li><li><code>jac_cache = nothing</code>: pre-allocated Jacobian cache for calls to <code>nlsolve</code> during the numerical algorithms</li><li><code>sparsity_detection::Bool = false</code>: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.</li></ul><p>The solution algorithms all use <code>nlsolve</code> to calculate the solution to systems of nonlinear equations. The user can pass in any of the keyword arguments for <code>nlsolve</code> to adjust the settings of the nonlinear solver.</p><p>For the keywords relevant to specific methods, see the docstring for the underlying method being called. Note these methods are not exported.</p><ul><li><code>:relaxation</code> -&gt; <code>relaxation!</code></li><li><code>:homotopy</code> -&gt; <code>homotopy!</code></li><li><code>:deterministic</code> -&gt; <code>deterministic_steadystate!</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/numerical_algorithms/solve.jl#L1-L58">source</a></section></article><h2 id="relaxation-1"><a class="docs-heading-anchor" href="#relaxation-1">Relaxation</a><a class="docs-heading-anchor-permalink" href="#relaxation-1" title="Permalink"></a></h2><p>The first and default numerical algorithm is a relaxation algorithm. The key problem in solving the equations characterizing <span>$(z, y, \Psi)$</span> is that it is difficult to jointly solve the nonlinear matrix equation for <span>$\Psi$</span> along with the steady-state equations for <span>$z$</span> and <span>$y$</span> due to the presence of the entropy term. The relaxation algorithm splits the solution of these equations into two steps, which allows us to calculate guesses of <span>$\Psi$</span> using linear algebra. It is in this sense that this iterative algorithm is a relaxation algorithm.</p><p>The system of equations characterizing the coefficients <span>$(z, y, \Psi)$</span> are solved iteratively in two separate steps. Given previous guesses <span>$(z_{n - 1}, y_{n - 1}, \Psi_{n - 1})$</span>, we calculate <span>$(z_n, y_n)$</span> such that</p><div>\[\begin{aligned}
0 &amp; = \mu(z_n, y_n) - z_n,\\
0 &amp; = \xi(z_n, y_n) + \Gamma_5 z_n + \Gamma_6 y_n + \mathcal{V}(z_{n - 1}),\\
\end{aligned}\]</div><p>is satisfied. In other words, we hold the entropy term constant and update <span>$(z_n, y_n)$</span> in the remaining terms. The coefficients are solved efficiently through <code>nlsolve</code> with <span>$(z_{n - 1}, y_{n - 1})$</span> as initial guesses.</p><p>Then we compute <span>$\Psi_n$</span> by solving</p><div>\[\begin{aligned}
0 &amp; = \Gamma_3 + \Gamma_4 \Psi_n + (\Gamma_5 + \Gamma_6 \Psi_n)(\Gamma_1 + \Gamma_2 \Psi_n) + J\mathcal{V}(z_{n - 1}).
\end{aligned}\]</div><p>with a <a href="https://en.wikipedia.org/wiki/Schur_decomposition#Generalized_Schur_decomposition">Generalized Schur decomposition</a> (also known as QZ decomposition). Notice that we also hold the Jacobian of the entropy constant. Only after we have a new round of <span>$(z_n, y_n, \Psi_n)$</span> do we update the entropy-related terms.</p><p>Convergence is achieved once <span>$(z_n, y_n, \Psi_n)$</span> are sufficiently close under some norm. By default, we use the <span>$L^\infty$</span> norm (maximum absolute error).</p><h2 id="homotopy-1"><a class="docs-heading-anchor" href="#homotopy-1">Homotopy Continuation</a><a class="docs-heading-anchor-permalink" href="#homotopy-1" title="Permalink"></a></h2><p>When the deterministic steady state exists, it is typically an easy problem to solve numerically. We can therefore use the equations characterizing the deterministic steady state for a <a href="https://en.wikipedia.org/wiki/Numerical_algebraic_geometry">homotopy continuation method</a>. Let <span>$q$</span> be the embedding parameter. Then the homotopy continuation method iteratively solves</p><div>\[\begin{aligned}
0 &amp; = \mu(z, y) - z,\\
0 &amp; = \xi(z, y) + \Gamma_5 z + \Gamma_6 y + q \mathcal{V}(z),\\
0 &amp; = \Gamma_3 + \Gamma_4 \Psi + (\Gamma_5 + \Gamma_6 \Psi)(\Gamma_1 + \Gamma_2 \Psi) + q J\mathcal{V}(z)
\end{aligned}\]</div><p>for the coefficients <span>$(z_q, y_q, \Psi_q)$</span> by increasing <span>$q$</span> from 0 to 1.</p><h2 id="blanchard-kahn-1"><a class="docs-heading-anchor" href="#blanchard-kahn-1">Blanchard-Kahn Conditions</a><a class="docs-heading-anchor-permalink" href="#blanchard-kahn-1" title="Permalink"></a></h2><p>At the end of <code>solve!</code>, we check the stochastic steady state found is locally unique and saddle-path stable by checking what are known as the Blanchard-Kahn conditions. Standard references for computational macroeconomics explain what these conditions are, so we defer to them (e.g. <a href="http://dept.ku.edu/~empirics/Emp-Coffee/blanchard-kahn_eca80.pdf">Blanchard-Kahn (1980)</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.8685&amp;rep=rep1&amp;type=pdf">Klein (2000)</a>, and <a href="https://link.springer.com/article/10.1023/A:1020517101123">Sims (2002)</a>). For the stochastic steady state, these conditions are essentially identical to the conditions for the deterministic steady state, but the Jacobian of the expectational equations to <span>$z_t$</span> also includes the Jacobian of the entropy. In the deterministic steady state, the entropy is zero, hence the Jacobian of the entropy is zero. In the stochastic steady state, the entropy is no longer zero and varies with <span>$z_t$</span>, hence the Jacobian of the expectational equations to <span>$z_t$</span> depends on entropy.</p><h2 id="sparsity-numerical-algo-1"><a class="docs-heading-anchor" href="#sparsity-numerical-algo-1">Exploiting Sparsity</a><a class="docs-heading-anchor-permalink" href="#sparsity-numerical-algo-1" title="Permalink"></a></h2><p>When solving for the deterministic or stochastic steady state, this package solves a system of nonlinear equations by calling <code>nlsolve</code>, whose underlying algorithms typically require calculating the Jacobian of the system of nonlinear equations. For many economic models, this system is sparse because each individual equation usually depends on a small subset of the coefficients <span>$(z, y, \Psi)$</span>. To exploit this sparsity and further accelerate computation time, we can use methods implemented by <a href="https://github.com/JuliaDiff/SparseDiffTools.jl">SparseDiffTools.jl</a>. For an example, please see this <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods/sparse_nlsolve_jacobians.jl">script</a>.</p><p>We automate the setup process by letting the user pass the keyword <code>sparse_jacobian = true</code> to <code>solve!</code>. If this keyword is true, then there are three ways to exploit sparsity.</p><ol><li><p>If no other keywords are passed, then <code>solve!</code> will attempt to determine the sparsity pattern. By default, the sparsity pattern is determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero. If the keyword <code>sparsity_detection = true</code>, then <code>solve!</code> will try using <a href="https://github.com/JuliaDiff/SparsityDetection.jl">SparsityDetection.jl</a>. Currently, the latter approach does not work with RiskAdjustedLinearizations.jl.</p></li><li><p>The keyword <code>sparsity</code> can be used to specify the sparsity pattern of the Jacobian. If <code>colorvec</code> is not also passed, then the matrix coloring vector is computed based on <code>sparsity</code>.</p></li><li><p>The keyword <code>jac_cache</code> allows the user to specify the sparsity pattern of the Jacobian and additionally pre-allocate the Jacobian&#39;s cache, which potentially achieves speed gains by avoiding extra allocations when the Jacobian function is repeatedly constructed.</p></li></ol><p>If <code>solve!</code> is called once, then the first two approaches are essentially the same. If <code>solve!</code> is repeatedly called (e.g. if the model&#39;s parameters are changed), then the second two approaches are strictly faster because computing the sparsity pattern or pre-allocating the Jacobian&#39;s cache only needs to be done once, as long as the system of equations does not change.</p><p>To simplify using the <code>sparsity</code>, <code>colorvec</code>, and <code>jac_cache</code> keywords, we implement two helper functions, <code>compute_sparsity_pattern</code> and <code>preallocate_jac_cache</code>. The first function calculates <code>sparsity</code> and <code>colorvec</code> while the second ones computes <code>jac_cache</code>. See the docstrings below and this <a href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/tree/main/examples/sparse_methods/sparse_nlsolve_jacobians.jl">example</a> for more details.</p><p>Some additional caveats on these methods:</p><ul><li>Creating a cached Jacobian with automatic differentiation via <code>ForwardColorJacCache</code> will not work because the objective function changes in each loop of the homotopy and relaxation algorithms, so the cached <code>Dual</code> matrices will have information on the wrong function after a loop completes. Currently, RiskAdjustedLinearizations.jl has not implemented a way to update the information on the function required by the <code>Dual</code> matrices.</li><li>If automatic differentiation does not work with dense Jacobians due to problems with reinterpreting the chunk size, then it will also not work when using sparse Jacobians.</li></ul><h2 id="Docstrings-1"><a class="docs-heading-anchor" href="#Docstrings-1">Docstrings</a><a class="docs-heading-anchor-permalink" href="#Docstrings-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.relaxation!" href="#RiskAdjustedLinearizations.relaxation!"><code>RiskAdjustedLinearizations.relaxation!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">relaxation!(ral, xₙ₋₁, Ψₙ₋₁; tol = 1e-10, max_iters = 1000, damping = .5, pnorm = Inf,
            schur_fnct = schur!, autodiff = :central, use_anderson = false, m = 5,
            verbose = :none, kwargs...)</code></pre><p>solves for the coefficients <span>$(z, y, \Psi)$</span> of a risk-adjusted linearization by the following relaxation algorithm:</p><ol><li><p>Initialize guesses for <span>$(z, y, \Psi)$</span></p></li><li><p>Do until convergence</p><p>a) Solve for <span>$(z, y)$</span> using the expectational and state transition equations and fixing <span>$\Psi$</span>.</p><p>b) Use a QZ decomposition to solve for <span>$\Psi$</span> while fixing <span>$(z, y)$</span>.</p></li></ol><p><strong>Types:</strong></p><ul><li><code>S1 &lt;: Number</code></li><li><code>S2 &lt;: Real</code></li><li><code>S3 &lt;: Real</code></li></ul><p><strong>Inputs</strong></p><ul><li><code>m::RiskAdjustedLinearization</code>: object holding functions needed to calculate   the risk-adjusted linearization</li><li><code>xₙ₋₁::AbstractVector{S1}</code>: initial guess for <span>$(z, y)$</span></li><li><code>Ψₙ₋₁::AbstractVector{S1}</code>: initial guess for <span>$\Psi$</span></li></ul><p><strong>Keywords</strong></p><ul><li><code>tol::S2</code>: convergence tolerance of residual norm for relaxation algorithm</li><li><code>max_iters::Int</code>: maximumm number of iterations</li><li><code>damping::S2</code>: guesses are updated as the weighted average   <code>xₙ = damping * proposal + (1 - damping) * xₙ₋₁</code>.</li><li><code>pnorm::S3</code>: norm for residual tolerance</li><li><code>schur_fnct::Function</code>: function for calculating the Schur factorization during QZ decomposition</li><li><code>autodiff::Symbol</code>: specifies whether to use autoamtic differentiation in <code>nlsolve</code>   (and is the same keyword as the <code>autodiff</code> keyword for <code>nlsolve</code>)</li><li><code>use_anderson::Bool</code>: set to true to apply Anderson acceleration to the   fixed point iteration of the relaxation algorithm</li><li><code>m::Int</code>: <code>m</code> coefficient if using Anderson acceleration</li><li><code>sparse_jacobian::Bool = false</code>: if true, exploit sparsity in the Jacobian in calls to <code>nlsolve</code> using SparseDiffTools.jl.   If <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>, then <code>relaxation!</code> will attempt to determine the sparsity pattern.</li><li><code>sparsity::Union{AbstractArray, Nothing} = nothing</code>: sparsity pattern for the Jacobian in calls to <code>nlsolve</code></li><li><code>colorvec = nothing</code>: matrix coloring vector for sparse Jacobian in calls to <code>nlsolve</code></li><li><code>jac_cache = nothing</code>: pre-allocated Jacobian cache for calls to <code>nlsolve</code> during the numerical algorithms</li><li><code>sparsity_detection::Bool = false</code>: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.</li><li><code>verbose::Symbol</code>: verbosity of information printed out during solution.   a) <code>:low</code> -&gt; statement when homotopy continuation succeeds   b) <code>:high</code> -&gt; statement when homotopy continuation succeeds and for each successful iteration</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/numerical_algorithms/relaxation.jl#L1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.homotopy!" href="#RiskAdjustedLinearizations.homotopy!"><code>RiskAdjustedLinearizations.homotopy!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">homotopy!(m, xₙ₋₁; step = .1, pnorm = Inf, verbose = :none, kwargs...)</code></pre><p>solves the system of equations characterizing a risk-adjusted linearization by a homotopy method with embedding parameter <span>$q$</span>, which steps from 0 to 1, with <span>$q = 1$</span> obtaining the true solution.</p><p>Currently, the only algorithm for choosing <span>$q$</span> is a simple uniform step search. Given a step size <span>$\Delta$</span>, we solve the homotopy starting from <span>$q = \Delta$</span> and increase <span>$q$</span> by <span>$\Delta$</span> until <span>$q$</span> reaches 1 or passes 1 (in which case, we force <span>$q = 1$</span>).</p><p><strong>Types:</strong></p><ul><li><code>S1 &lt;: Number</code></li></ul><p><strong>Inputs</strong></p><ul><li><code>m::RiskAdjustedLinearization</code>: object holding functions needed to calculate   the risk-adjusted linearization</li><li><code>xₙ₋₁::AbstractVector{S1}</code>: initial guess for <span>$(z, y, \Psi)$</span></li></ul><p><strong>Keywords</strong></p><ul><li><code>step::Float64</code>: size of the uniform step from <code>step</code> to 1.</li><li><code>pnorm::Float64</code>: norm under which to evaluate the errors after homotopy succeeds.</li><li><code>sparse_jacobian::Bool = false</code>: if true, exploit sparsity in the Jacobian in calls to <code>nlsolve</code> using SparseDiffTools.jl.   If <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>, then <code>homotopy!</code> will attempt to determine the sparsity pattern.</li><li><code>sparsity::Union{AbstractArray, Nothing} = nothing</code>: sparsity pattern for the Jacobian in calls to <code>nlsolve</code></li><li><code>colorvec = nothing</code>: matrix coloring vector for sparse Jacobian in calls to <code>nlsolve</code></li><li><code>jac_cache = nothing</code>: pre-allocated Jacobian cache for calls to <code>nlsolve</code> during the numerical algorithms</li><li><code>sparsity_detection::Bool = false</code>: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both <code>jac_cache</code> and <code>sparsity</code> are <code>nothing</code>). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.</li><li><code>verbose::Symbol</code>: verbosity of information printed out during solution.   a) <code>:low</code> -&gt; statement when homotopy continuation succeeds   b) <code>:high</code> -&gt; statement when homotopy continuation succeeds and for each successful iteration</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/numerical_algorithms/homotopy.jl#L1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.blanchard_kahn" href="#RiskAdjustedLinearizations.blanchard_kahn"><code>RiskAdjustedLinearizations.blanchard_kahn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">blanchard_kahn(m::RiskAdjustedLinearization; deterministic::Bool = false, verbose::Symbol = :high)</code></pre><p>checks the Blanchard-Kahn conditions for whether a first-order perturbation is saddle-path stable or not.</p><p>If <code>verbose</code> is <code>:low</code> or <code>:high</code>, a print statement will be shown if the Blanchard-Kahn conditions are satisfied.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/numerical_algorithms/blanchard_kahn.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.compute_sparsity_pattern" href="#RiskAdjustedLinearizations.compute_sparsity_pattern"><code>RiskAdjustedLinearizations.compute_sparsity_pattern</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">compute_sparsity_pattern(f::Function, x::AbstractVector{&lt;: Number}, nrow::Int;
                         sparsity::Union{AbstractArray, Nothing} = nothing,
                         sparsity_detection::Bool = false)</code></pre><p>calculates the sparsity pattern of the Jacobian of the functions μ, ξ, and 𝒱.</p><p><strong>Inputs</strong></p><ul><li><code>f</code>: is the function to be differentiated, e.g. <code>z -&gt; 𝒱(z, Ψ, (1, 2))</code></li><li><code>x</code>: the vector at which differentiation occurs</li><li><code>nrow</code>: specifies the number of rows of the Jacobian</li></ul><p><strong>Keywords</strong></p><ul><li><code>sparsity</code>: sparsity pattern of the Jacobian</li><li><code>sparsity_detection</code>: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using automatic differentiation   to calculate a Jacobian and assuming any zeros are supposed to be zero.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/sparse_jacobian_helpers.jl#L3-L21">source</a></section><section><div><pre><code class="language-none">compute_sparsity_pattern(m::RiskAdjustedLinearization, algorithm::Symbol; q::Float64 = .1,
                         sparsity::Union{AbstractArray, Nothing} = nothing,
                         sparsity_detection::Bool = false)</code></pre><p>calculates the sparsity pattern and matrix coloring vector of the Jacobian of the nonlinear system of equations for either the deterministic or stochastic steady state, depending on which <code>algorithm</code> is called.</p><p><strong>Keywords</strong></p><ul><li><code>q</code>: step size for homotopy. Should satisfy <code>0 &lt; q &lt; 1</code> and is only required to ensure   that the sparsity pattern is correctly determined when <code>algorithm = :homotopy</code>   and thus the dependence of the entropy <code>𝒱</code> on the coefficients <code>(z, y, Ψ)</code> matters.</li><li><code>sparsity</code>: sparsity pattern of the Jacobian of the nonlinear system of equations</li><li><code>sparsity_detection</code>: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using finite differences   to calculate a Jacobian and assuming any zeros are supposed to be zero.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/sparse_jacobian_helpers.jl#L415-L433">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RiskAdjustedLinearizations.preallocate_jac_cache" href="#RiskAdjustedLinearizations.preallocate_jac_cache"><code>RiskAdjustedLinearizations.preallocate_jac_cache</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">preallocate_jac_cache(m::RiskAdjustedLinearization, algorithm::Symbol; q::Float64 = .1,
                      sparsity::Union{AbstractArray, Nothing} = nothing,
                      sparsity_detection::Bool = false)</code></pre><p>pre-allocates the cache for the Jacobian of the nonlinear system of equations for either the deterministic or stochastic steady state, depending on which <code>algorithm</code> is called.</p><p><strong>Keywords</strong></p><ul><li><code>q</code>: step size for homotopy. Should satisfy <code>0 &lt; q &lt; 1</code> and is only required to ensure   that the sparsity pattern is correctly determined when <code>algorithm = :homotopy</code>   and thus the dependence of the entropy <code>𝒱</code> on the coefficients <code>(z, y, Ψ)</code> matters.</li><li><code>sparsity</code>: the sparsity pattern of the Jacobian of the nonlinear system of equations</li><li><code>sparsity_detection</code>: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using finite differences   to calculate a Jacobian and assuming any zeros are supposed to be zero.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chenwilliam77/RiskAdjustedLinearizations.jl/blob/2d8910d1dabfc7b969c8d048f82e7351eb350edf/src/sparse_jacobian_helpers.jl#L457-L475">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sparse_arrays_jacs/">« Sparse Arrays and Jacobians</a><a class="docs-footer-nextpage" href="../example/">Example »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 29 January 2021 02:01">Friday 29 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
